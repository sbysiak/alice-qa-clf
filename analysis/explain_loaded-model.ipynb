{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "import pyod\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.loci import LOCI\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.sos import SOS\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "shap.initjs()\n",
    "from helper.pdp import compute_pdp, plot_pdp, plot_ice\n",
    "\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "import dill\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\", category=FutureWarning, module='pyod')\n",
    "warnings.filterwarnings(\"default\", category=FutureWarning, module='sklearn')\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = dill.load(open('storage/model_IF-100_k30.dill', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf            = model_data['clf']\n",
    "X              = model_data['X']\n",
    "y              = model_data['y']\n",
    "scaler         = model_data['scaler']\n",
    "col_names      = model_data['col_names']\n",
    "shap_explainer = model_data['shap_explainer']\n",
    "shap_values    = model_data['shap_values'] \n",
    "k_in_kmeans    = model_data['k_in_kmeans']\n",
    "description    = model_data['description']\n",
    "fname_df_orig  = model_data['fname_df_orig']\n",
    "scores         = model_data['scores']\n",
    "\n",
    "assert all(shap_explainer.model.f(X) == scores)\n",
    "df_orig = pd.read_csv(fname_df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(shap_explainer.model.f(X) - scores).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_idx = y == 0\n",
    "bad_idx  = y == 1\n",
    "\n",
    "scores = clf.predict_proba(X, method='linear')[:,1]\n",
    "# stat_descr(scores, quantiles=[0, 0.05, 0.25, 0.5, 0.75, 0.95, 1.])\n",
    "\n",
    "bins = np.histogram(scores, 30)[1]\n",
    "fig, axes = plt.subplots(2,2, figsize=(14,10))\n",
    "# axes[0].hist(scores, bins=bins, histtype='step', lw=2, density=1, color='k');\n",
    "\n",
    "axes[0][0].hist(scores, bins=bins, histtype='step', lw=2, density=1, color='k');\n",
    "axes[0][0].set_title('normalized');\n",
    "\n",
    "axes[0][1].hist(scores, bins=bins, histtype='step', lw=2, density=0, color='k');\n",
    "axes[0][1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[0][1].set_title('unnormalized (log y)');\n",
    "\n",
    "axes[1][0].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=1, color='b');\n",
    "axes[1][0].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=1, color='r');\n",
    "axes[1][0].set_title('normalized by class');\n",
    "\n",
    "axes[1][1].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=0, color='b');\n",
    "axes[1][1].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=0, color='r');\n",
    "axes[1][1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[1][1].set_title('unnormalized (log y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~Permutation importance~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation importance requires metric which is evaluated before and after permutation (drops in this metric correspond to feature importance).\n",
    "\n",
    "One could think about permutation importance using raw scores returned by model,  \n",
    "e.g. (`predict_proba` in `sklearn` or `decision_function` in `PyOD`) but it's not standard way of computing permutation importances.  \n",
    "As single value is required to compute deviation from the baseline, one should use sth like:  \n",
    "`aver( permutated_scores - original_scores )`  \n",
    "The problem is that if features were independent then the importance of any feature will be equal to zero (contributions to various points' predictions will vanish in total) and it's very incorrect.  \n",
    "Possible workaround would be to use `abs(permutated_scores - original_scores)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def permutation_importances_custom(score_func, X):\n",
    "#     baseline = score_func(X)\n",
    "#     imp = []\n",
    "#     imp_abs = []\n",
    "#     for col in X.columns:\n",
    "#         save = X[col].copy()\n",
    "#         X[col] = np.random.permutation(X[col])\n",
    "#         m = score_func(X)\n",
    "#         X[col] = save\n",
    "#         imp.append( np.mean(baseline - m) )\n",
    "#         imp_abs.append( np.mean(abs(baseline - m)) )\n",
    "#     return np.array(imp), np.array(imp_abs), X.columns.to_numpy()\n",
    "\n",
    "\n",
    "# x1 = np.random.random_sample(1000)\n",
    "# x2 = np.random.random_sample(1000)\n",
    "# y = x1 + x2\n",
    "\n",
    "# y = data_s['bad']\n",
    "# X = data_s.drop('bad', axis=1)\n",
    "# fimps, fimps_abs, fnames = permutation_importances_custom(score, X)\n",
    "# idx = np.argsort(fimps_abs)\n",
    "# idx.reverse()\n",
    "# # idx = [i for i,_ in enumerate(fnames)]\n",
    "# # for name, imp in zip(fnames, fimps): \n",
    "# for name, imp, imp_abs in zip(fnames[idx], fimps[idx], fimps_abs[idx]): \n",
    "#     print(f'{name:>25s}, {imp:>10.5f}, {imp_abs:>10.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most important variables from SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, \n",
    "                  X, \n",
    "                  feature_names=col_names,\n",
    "                  plot_type='bar'\n",
    "                 )\n",
    "sorted_features_shap = col_names[abs(shap_values).mean(axis=0).argsort()[::-1]].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced SHAP analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, \n",
    "                  X, \n",
    "#                   plot_type='bar'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('meanVertX', \n",
    "                shap_values, \n",
    "                X, \n",
    "                feature_names=col_names,\n",
    "#                 interaction_index='fConstituents-0.fpT',\n",
    "                interaction_index=None,\n",
    "#                 xmin=-6, xmax=5,\n",
    "                alpha=0.3\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(shap_explainer.expected_value, \n",
    "                shap_values, \n",
    "                X, \n",
    "                feature_names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def partial_dependence_aver(clf, X, feat_name, percentiles_range=[0, 100], n_grid_points=50, scaler=None):\n",
    "#     \"\"\"computes partial dependence\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     scaler : sklearn.preprocessing.StandardScaler or None\n",
    "#         scaler obj to compute inverse transformation of `X` \n",
    "#         if None, then no transformation of X is computed\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     grid : array, shape [n_grid_points]\n",
    "#         values of `feat_name` for which partial dependece was computed\n",
    "#         (x axis for pdp)\n",
    "#     aver_score : array, shape [n_grid_points]\n",
    "#         average score for samples with inputed values of `feat_name`\n",
    "#         (y axis for pdp)\n",
    "#     \"\"\"\n",
    "#     X_temp = X.copy()\n",
    "    \n",
    "#     grid = np.linspace(np.percentile(X_temp[feat_name], percentiles_range[0]),\n",
    "#                        np.percentile(X_temp[feat_name], percentiles_range[1]),\n",
    "#                        n_grid_points)\n",
    "    \n",
    "#     aver_pred = np.zeros(n_grid_points)\n",
    "#     aver_score = np.zeros(n_grid_points)\n",
    "    \n",
    "#     orig_pred_aver  = np.average(clf.predict(X))\n",
    "#     orig_pred_std   = np.std(clf.predict(X))\n",
    "#     orig_score_aver = np.average(clf.decision_function(X))\n",
    "#     orig_score_std  = np.std(clf.decision_function(X))\n",
    "    \n",
    "#     for i, val in enumerate(grid):\n",
    "#         X_temp[feat_name] = val\n",
    "#         aver_pred[i]  = (np.average(clf.predict(X_temp)) - orig_pred_aver) / orig_pred_std\n",
    "#         aver_score[i] = (np.average(clf.decision_function(X_temp)) - orig_score_aver) / orig_score_std\n",
    "        \n",
    "#         if not i % 5: print(i, end=', ')\n",
    "#     print()\n",
    "            \n",
    "#     if scaler:\n",
    "#         feat_id = X.columns.tolist().index(feat_name)\n",
    "#         grid_mat = np.zeros([n_grid_points, X.shape[1]])\n",
    "#         grid_mat[:, feat_id] = grid\n",
    "#         print(feat_id)\n",
    "#         grid_mat_tr = scaler.inverse_transform(grid_mat)\n",
    "#         grid = grid_mat_tr[:, feat_id]\n",
    "            \n",
    "# #         feat_mean = scaler.mean_[feat_id]\n",
    "# #         feat_var  = scaler.var_[feat_id]\n",
    "# #         grid      = grid * feat_var + feat_mean\n",
    "        \n",
    "#     return grid, aver_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ncols, nrows = 3,4\n",
    "features = sorted_features_shap\n",
    "\n",
    "def score_func(X,y):\n",
    "    # does not use `y`\n",
    "    return clf.predict_proba(X, method='linear')[:,1]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(20,15))\n",
    "for r in range(nrows):\n",
    "    for c in range(ncols):\n",
    "        i = r*ncols + c\n",
    "        feat_name = features[i]\n",
    "        print(i)\n",
    "        result = compute_pdp(score_func, \n",
    "                             X=X, \n",
    "                             feat_name=feat_name, \n",
    "                             quantiles_range=[0.005, 0.995],\n",
    "                             n_grid_points=20, \n",
    "                             scaler=scaler)\n",
    "\n",
    "        plot_pdp(result, \n",
    "                 plot_rugs=True, \n",
    "#                  plot_aver=True,\n",
    "        #          instance_to_highlight=15,\n",
    "                 centered=True, percentile_center_at=50, \n",
    "                 plot_lines=True, lines_frac_to_plot=100,\n",
    "                 plot_errorband=False, errorband_nsigma=2,\n",
    "                 ax=axes[r][c]\n",
    "                )\n",
    "        axes[r][c].set_title(feat_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# feature = 'meanMIP'\n",
    "\n",
    "# def score_func(X,y):\n",
    "#     # does not use `y`\n",
    "#     return clf.predict_proba(X, method='linear')[:,1]\n",
    "    \n",
    "# result = compute_pdp(score_func, \n",
    "#                      X=X, \n",
    "#                      feat_name=feature, \n",
    "#                      quantiles_range=[0.005, 0.995],\n",
    "#                      n_grid_points=40, \n",
    "#                      scaler=scaler)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8,5))\n",
    "# ax = plot_pdp(result, \n",
    "#          plot_rugs=True, \n",
    "#          centered=True, percentile_center_at=50, \n",
    "#          plot_lines=True, lines_frac_to_plot=100,\n",
    "#          plot_errorband=False, errorband_nsigma=2,\n",
    "#          ax=ax\n",
    "#         )\n",
    "\n",
    "# ax.set_title(f'margin dependence on {feature}');\n",
    "# fig.savefig('graphics/PDP.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(clf.decision_scores_)[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.argsort(clf.decision_scores_)[-200:]:\n",
    "    if y[i] == 0: print(i, '<--- flag OFF !!!')\n",
    "    else: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_index = 213\n",
    "\n",
    "row_orig = df_orig.iloc[instance_index]\n",
    "row = X.iloc[instance_index].to_numpy().reshape(1,-1)\n",
    "global_warning_flag = df_orig['alias_global_Warning'].iloc[instance_index]\n",
    "model_pred = clf.predict(row)[0]\n",
    "model_score = clf.decision_function(row)[0]\n",
    "model_prob_linear  = clf.predict_proba(row, method='linear')[0][1]\n",
    "model_prob_unify  = clf.predict_proba(row, method='unify')[0][1]\n",
    "\n",
    "score_percentile = percentileofscore(clf.decision_scores_, model_score)\n",
    "\n",
    "status_str =  f\"chunk {instance_index} [ {row_orig['period.fString']} / {row_orig['run']} / chunk {row_orig['chunkID']} ]:  \\n - _globalWarning_ flag set to: **{global_warning_flag:.0f}**  \\n - model prediction is: **{model_pred}**  \\n - model score: prob(lin) =  **{model_prob_linear:.3f}**, prob(unify) = {model_prob_unify:.3f} which is **{score_percentile:.2f}** percentile (higher = more outlier-like)\"\n",
    "printmd(status_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "222 - 99.8 perc out, flag on, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "N_SAMPLES = 10000\n",
    "\n",
    "n_top = 20\n",
    "n_rep = 3\n",
    "\n",
    "res = {}\n",
    "print(f'\\nsample id = {instance_index}')\n",
    "\n",
    "\n",
    "for i in range(n_rep):\n",
    "    print(i, end='...')\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(X.to_numpy(), feature_names=col_names, \n",
    "                                                       training_labels=y,\n",
    "                                                       class_names=['inlier', 'outlier'],\n",
    "                                                       discretize_continuous=False, discretizer='entropy')\n",
    "\n",
    "    exp = explainer.explain_instance(X.iloc[instance_index], \n",
    "                                     predict_fn=lambda X: clf.predict_proba(X, method='linear'), \n",
    "                                     num_features=50, num_samples=N_SAMPLES) # more features in order to fill values for less important feats also\n",
    "#     exp.show_in_notebook(show_table=True, show_all=False)\n",
    "#     exp.as_pyplot_figure()\n",
    "#     for feat, val in exp.as_list():\n",
    "#         if feat in res.keys(): res[feat].append(val)\n",
    "#         else: res[feat] = [val,]\n",
    "    for feat_id, val in exp.as_map()[1]:\n",
    "        feat = col_names[feat_id]\n",
    "        if feat in res.keys(): res[feat].append(val)\n",
    "        else: res[feat] = [val,]\n",
    "\n",
    "# PRINT RESULTS\n",
    "print()\n",
    "sorted_keys = sorted(res.keys(), key=lambda k: -abs(np.mean(res[k])))\n",
    "for k in sorted_keys[:n_top]:\n",
    "    v = res[k]\n",
    "    print(f'{k:>50s} : {np.mean(v):2.4f} +/- {np.std(v):.4f} (n={len(v)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats = [(k, np.mean(res[k]), np.std(res[k])) for k in sorted_keys[:n_top]]\n",
    "names = [xi[0] for xi in top_feats[:n_top]]\n",
    "means = [xi[1] for xi in top_feats[:n_top]]\n",
    "stds  = [xi[2] for xi in top_feats[:n_top]]\n",
    "colors = ['r' if xi[1]>0 else 'b' for xi in top_feats[:n_top]]\n",
    "\n",
    "plt.barh(range(n_top,0,-1), means, xerr=stds, color=colors)\n",
    "locs, _ = plt.yticks()\n",
    "plt.yticks(range(n_top,0,-1), names);\n",
    "\n",
    "top_feats_lime = deepcopy(top_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 20\n",
    "top_feats = sorted([el for el in zip(col_names, shap_values[instance_index,])], key=lambda x: -abs(x[1]))\n",
    "# print(x[:n_top])\n",
    "names  = [xi[0] for xi in top_feats[:n_top]]\n",
    "means  = [xi[1] for xi in top_feats[:n_top]]\n",
    "colors = ['r' if xi[1]>0 else 'b' for xi in top_feats[:n_top]]\n",
    "\n",
    "plt.barh(range(n_top,0,-1), means, color=colors)\n",
    "locs, _ = plt.yticks()\n",
    "plt.yticks(range(n_top,0,-1), names);\n",
    "\n",
    "top_feats_shap = deepcopy(top_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# explainer_kmeans = explainer_kmeans_10\n",
    "# shap_values_kmeans = shap_values_kmeans_10\n",
    "shap.initjs()\n",
    "shap.force_plot(shap_explainer.expected_value, \n",
    "                shap_values[instance_index,:], \n",
    "                X.iloc[instance_index], \n",
    "                feature_names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "top_feats = top_feats_shap\n",
    "\n",
    "n_cols, n_rows = 3, 5\n",
    "# qmin, qmax = 0.005, 0.995\n",
    "qmin, qmax = 0, 1\n",
    "n_bins = 50\n",
    "group_by_class = False\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16,4*n_rows))\n",
    "# plt.subplots?\n",
    "for c in range(n_cols):\n",
    "    for r in range(n_rows):\n",
    "        ax = axes[r][c]\n",
    "        i = r*n_cols + c\n",
    "        var_name = top_feats[i][0]\n",
    "#         if 'roc' in var_name: continue\n",
    "        minv = np.quantile(X[var_name], qmin)\n",
    "        maxv = np.quantile(X[var_name], qmax)\n",
    "        bins = np.linspace(minv, maxv, n_bins)\n",
    "        if group_by_class:\n",
    "            ax.hist(X[var_name][y==0], bins=bins, color='blue', histtype='step', density=1)\n",
    "            ax.hist(X[var_name][y==1], bins=bins, color='red', histtype='step', density=1)\n",
    "        else:\n",
    "            ax.hist(X[var_name], bins=bins, color='green', histtype='step', density=1)\n",
    "            \n",
    "        ax.set_title(var_name)\n",
    "        \n",
    "        inst_val = X[var_name][instance_index]\n",
    "        xrange = ax.get_xlim()[1] - ax.get_xlim()[0]\n",
    "        yrange = ax.get_ylim()[1] - ax.get_ylim()[0]\n",
    "        if inst_val > ax.get_xlim()[1] or inst_val < ax.get_xlim()[0]:\n",
    "            mid = ax.get_xlim()[1]/2 + ax.get_xlim()[0]/2\n",
    "            factor = 1 if inst_val > ax.get_xlim()[1] else -1\n",
    "            dx = 0.2*xrange * factor\n",
    "            arr_xpos = mid + 0.2*xrange*factor\n",
    "            txt_xpos = arr_xpos - 0.1*xrange*factor\n",
    "        else: \n",
    "            dx = 0\n",
    "            arr_xpos = inst_val\n",
    "            txt_xpos = arr_xpos + 0.1*xrange\n",
    "\n",
    "        \n",
    "        ax.arrow(arr_xpos, yrange, dx, -0.3*yrange, width=0.003*xrange, \n",
    "                length_includes_head=True, head_length=0.1*yrange, head_width=0.02*xrange,\n",
    "                fc='k')\n",
    "#         txt_xpos = inst_val+0.2 if (inst_val > minv and inst_val < maxv) else (maxv+minv)/1.5\n",
    "#         txt_xpos = arr_xpos+0.2\n",
    "        ax.text(txt_xpos, yrange*0.9, f'{inst_val:.3f}', fontsize=14, horizontalalignment='center')\n",
    "    \n",
    "        if top_feats[i][1] > 0: \n",
    "            ax.text(0.999, 1.2, '+', fontsize=30, color='red', transform=ax.transAxes, \n",
    "                    verticalalignment='top', horizontalalignment='right')\n",
    "        elif top_feats[i][1] < 0:\n",
    "            ax.text(0.98, 1.3, '-', fontsize=60, color='blue', transform=ax.transAxes, \n",
    "                    verticalalignment='top', horizontalalignment='right')\n",
    "#         axes[r][c].legend?\n",
    "    \n",
    "plt.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols, nrows = 3,5\n",
    "top_feats = top_feats_shap\n",
    "\n",
    "# qmin,qmax = 0,1\n",
    "\n",
    "def score_func(X,y):\n",
    "    # does not use `y`\n",
    "    return clf.predict_proba(X, method='linear')[:,1]\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16,4*n_rows))\n",
    "for r in range(nrows):\n",
    "    for c in range(ncols):\n",
    "        i = r*ncols + c\n",
    "        ax = axes[r][c]\n",
    "        feat_name = top_feats[i][0]\n",
    "        print(i)\n",
    "        result = compute_pdp(score_func, \n",
    "                             X=X, \n",
    "                             feat_name=feat_name, \n",
    "                             quantiles_range=[qmin, qmax],\n",
    "                             n_grid_points=20, \n",
    "                             scaler=scaler)\n",
    "\n",
    "        plot_ice(result, instance_to_highlight=instance_index,\n",
    "             plot_rugs=True, \n",
    "             plot_aver=False,\n",
    "             centered=True, percentile_center_at=50, \n",
    "             plot_lines=False, lines_frac_to_plot=50,\n",
    "             plot_errorband=False, errorband_nsigma=2,\n",
    "             ax=ax\n",
    "            )\n",
    "        ax.set_title(feat_name)\n",
    "        \n",
    "        if top_feats[i][1] > 0: \n",
    "            ax.text(0.999, 1.15, '+', fontsize=30, color='red', transform=ax.transAxes, \n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                   )\n",
    "        elif top_feats[i][1] < 0:\n",
    "            ax.text(0.98, 1.25, '-', fontsize=60, color='blue', transform=ax.transAxes, \n",
    "                    verticalalignment='top', horizontalalignment='right', \n",
    "                    )\n",
    "            \n",
    "plt.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature='meanMIP'\n",
    "# result = compute_pdp(score_func, \n",
    "#                      X=X, \n",
    "#                      feat_name=feature, \n",
    "#                      quantiles_range=[0.005, 0.995],\n",
    "#                      n_grid_points=40, \n",
    "#                      scaler=scaler)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8,5))\n",
    "# ax = plot_ice(result, instance_to_highlight=instance_index,\n",
    "#          plot_rugs=True,\n",
    "#          plot_aver=True,\n",
    "#          centered=True, percentile_center_at=50, \n",
    "#          plot_lines=True, lines_frac_to_plot=100,\n",
    "#          plot_errorband=False, errorband_nsigma=2,\n",
    "#          ax=ax\n",
    "#         )\n",
    "# ax.set_title(f'margin dependence on {feature}');\n",
    "# fig.savefig('graphics/ICE_lines_centered_aver.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display QA plots (per run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_plots = [('Event Information', 'TPC_event_info.png'), ('Cluster Occupancy', 'cluster_occupancy.png'), ('#eta, #phi and pt', 'eta_phi_pt.png'), ('Number of clusters in #eta and #phi', 'cluster_in_detail.png'), ('DCAs vs #eta', 'dca_in_detail.png'), ('TPC dEdx', 'TPC_dEdx_track_info.png'), ('DCAs vs #phi', 'dca_and_phi.png'), ('TPC-ITS matching', 'TPC-ITS.png'), ('dcar vs pT', 'dcar_pT.png'), ('Tracking parameter phi', 'pullPhiConstrain.png'), ('Raw QA Information', 'rawQAInformation.png'), ('Canvas ROC Status OCDB', 'canvasROCStatusOCDB.png'), ('Resolution vs pT and 1/pT', 'res_pT_1overpT.png'), ('Efficiency all charged + findable', 'eff_all+all_findable.png'), ('Efficiency #pi, K, p', 'eff_Pi_K_P.png'), ('Efficiency findable #pi, K, p', 'eff_Pi_K_P_findable.png')]\n",
    "file_names_mapping = dict(available_plots)\n",
    "\n",
    "row_orig = df_orig.iloc[instance_index]\n",
    "path = '/'.join([str(el) for el in row_orig[['year', 'period.fString', 'pass.fString', 'run']].to_list()])\n",
    "path = path.replace('pass1/', 'pass1/000')\n",
    "\n",
    "def show_qa_plot(plot_name, path=path, file_names_mapping=file_names_mapping):\n",
    "    plot_file_name = file_names_mapping[plot_name]\n",
    "    src = f\"http://aliqatpceos.web.cern.ch/aliqatpceos/data/{path}/{plot_file_name}\"\n",
    "    html = f'<img src={src} width=\"1200\" height=\"1200\">'\n",
    "    print(src)\n",
    "    return md(html)\n",
    "    \n",
    "\n",
    "interact(show_qa_plot, plot_name=file_names_mapping.keys(), \n",
    "         path=fixed(path), file_names_mapping=fixed(file_names_mapping));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_plots = [('Event Information', 'TPC_event_info.png'), ('Cluster Occupancy', 'cluster_occupancy.png'), ('#eta, #phi and pt', 'eta_phi_pt.png'), ('Number of clusters in #eta and #phi', 'cluster_in_detail.png'), ('DCAs vs #eta', 'dca_in_detail.png'), ('TPC dEdx', 'TPC_dEdx_track_info.png'), ('DCAs vs #phi', 'dca_and_phi.png'), ('TPC-ITS matching', 'TPC-ITS.png'), ('dcar vs pT', 'dcar_pT.png'), ('Tracking parameter phi', 'pullPhiConstrain.png'), ('Raw QA Information', 'rawQAInformation.png'), ('Canvas ROC Status OCDB', 'canvasROCStatusOCDB.png'), ('Resolution vs pT and 1/pT', 'res_pT_1overpT.png'), ('Efficiency all charged + findable', 'eff_all+all_findable.png'), ('Efficiency #pi, K, p', 'eff_Pi_K_P.png'), ('Efficiency findable #pi, K, p', 'eff_Pi_K_P_findable.png')]\n",
    "file_names_mapping = dict(available_plots)\n",
    "\n",
    "row_orig = df_orig.iloc[instance_index]\n",
    "path = '/'.join([str(el) for el in row_orig[['year', 'period.fString', 'pass.fString', 'run']].to_list()])\n",
    "path = path.replace('pass1/', 'pass1/000')\n",
    "\n",
    "def show_qa_plot(plot_name, path=path, file_names_mapping=file_names_mapping):\n",
    "    plot_file_name = file_names_mapping[plot_name]\n",
    "    return md(f\"![QA](http://aliqatpceos.web.cern.ch/aliqatpceos/data/{path}/{plot_file_name})\")\n",
    "\n",
    "\n",
    "# interact(show_qa_plot, plot_name=file_names_mapping.keys(), \n",
    "#          path=fixed(path), file_names_mapping=fixed(file_names_mapping));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = \"\"\"<div align=\"left\"><br>\n",
    "# <h2>Run Data Quality</h2>\n",
    "# <a href=\"TPC_event_info.png\">Event Information</a><br>\n",
    "# <a href=\"cluster_occupancy.png\">Cluster Occupancy</a><br>\n",
    "# <a href=\"eta_phi_pt.png\">#eta, #phi and pt</a><br>\n",
    "# <a href=\"cluster_in_detail.png\">Number of clusters in #eta and #phi</a><br>\n",
    "# <a href=\"dca_in_detail.png\">DCAs vs #eta</a><br>\n",
    "# <a href=\"TPC_dEdx_track_info.png\">TPC dEdx</a><br>\n",
    "# <a href=\"dca_and_phi.png\">DCAs vs #phi</a><br>\n",
    "# <a href=\"TPC-ITS.png\">TPC-ITS matching</a><br>\n",
    "# <a href=\"dcar_pT.png\">dcar vs pT</a><br>\n",
    "# <a href=\"pullPhiConstrain.png\">Tracking parameter phi</a><br>\n",
    "# <a href=\"rawQAInformation.png\">Raw QA Information</a><br>\n",
    "# <a href=\"canvasROCStatusOCDB.png\">Canvas ROC Status OCDB</a><br>\n",
    "# <!--  <a href=\"res_pT_1overpT.png\">Resolution vs pT and 1/pT</a><br>  //-->\n",
    "# <!--  <a href=\"eff_all+all_findable.png\">Efficiency all charged + findable</a><br>  //-->\n",
    "# <!--  <a href=\"eff_Pi_K_P.png\">Efficiency #pi, K, p</a><br>  //-->\n",
    "# <!--  <a href=\"eff_Pi_K_P_findable.png\">Efficiency findable #pi, K, p</a><br>  //-->\n",
    "# </div>\"\"\"\n",
    "\n",
    "# img_names = []\n",
    "# for line in h.split('<br>'):\n",
    "#     print(line)\n",
    "#     if 'href' in line:\n",
    "#         img_file_name = line.split('\"')[1]\n",
    "#         img_name = line.split('png\">')[1].split('<')[0]\n",
    "#         print('img_name = ',img_name, '>>', img_file_name)\n",
    "#         img_names.append((img_name, img_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- investigate usage of PyOD_clf.predict_proba(X, method='unify') -- everywhere (also in issues below)\n",
    "- investigate LIME's parameters\n",
    "- investigate SHAP coherence for various `k` in k-means and various explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
