{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "import pyod\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.loci import LOCI\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.sos import SOS\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "shap.initjs()\n",
    "from helper.pdp import compute_pdp, plot_pdp, plot_ice\n",
    "\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "import dill\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\", category=FutureWarning, module='pyod')\n",
    "warnings.filterwarnings(\"default\", category=FutureWarning, module='sklearn')\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_descr(arr, quantiles=[0, 0.25, 0.5, 0.75, 1.]):       \n",
    "    n = len(arr)\n",
    "    mu = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    q_vals = np.quantile(arr, quantiles)\n",
    "    \n",
    "    def custom_format(x):\n",
    "        if x > 50: return '{:^12.0f}'.format(x)\n",
    "        else: return '{:^12.2f}'.format(x)\n",
    "    q_str_perc = (''+'|').join(['{:8.0f}%   '.format(q*100) for q in quantiles])\n",
    "    q_str_vals = (''+'|').join([custom_format(q) for q in q_vals])\n",
    "    line = '-------------'*len(quantiles) \n",
    "    print(f'{n} values\\n{mu:.3f} +/- {std:.3f}\\n{q_str_perc}\\n{line}\\n{q_str_vals}\\n')\n",
    "    \n",
    "# x = (np.random.randn(100)+5)\n",
    "# stat_descr(x)\n",
    "# stat_descr(x, quantiles=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "fname_df_orig = 'data/trending_merged_LHC18q_withGraphs.csv'\n",
    "df_orig = pd.read_csv(fname_df_orig)\n",
    "\n",
    "target_col = 'alias_global_Warning'\n",
    "#----------\n",
    "\n",
    "df = df_orig[[c for c in df_orig.columns if \n",
    "              ('gr' not in c and 'alias' not in c and 'Unnamed' not in c)\n",
    "              and c != 'dataType.fString'\n",
    "              or c == target_col\n",
    "             ]]\n",
    "rename = lambda c: c if c != target_col else 'bad'\n",
    "df.columns = [rename(c) for c in df.columns]\n",
    "\n",
    "good_idx = df['bad'] == 0\n",
    "bad_idx  = df['bad'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_vars = pd.read_csv('data/sensitive_variables_list.csv', sep=':')\n",
    "sens_vars = [var[:-2] for var in sens_vars] + ['bad']\n",
    "\n",
    "df = df[sens_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# out_det = KNN(n_neighbors=3, method='mean')  # method, n_neighbours\n",
    "\n",
    "\n",
    "data = df.drop(['bad'], axis=1)\n",
    "data = data.drop(['oroc_A_side', 'oroc_C_side', 'iroc_A_side', 'iroc_C_side'], axis=1)\n",
    "x = data.to_numpy()\n",
    "scaler = StandardScaler(with_mean=False, with_std=False) \n",
    "x_s = scaler.fit_transform(x)\n",
    "# data_s = (data - data.mean()) / data.std()\n",
    "X = pd.DataFrame(x_s, columns=data.columns)\n",
    "y = df['bad']\n",
    "col_names = X.columns\n",
    "\n",
    "# out_det = ABOD(n_neighbors=15)\n",
    "# out_det = CBLOF()\n",
    "# out_det = HBOS()\n",
    "# out_det = IForest(100, contamination=0.04)\n",
    "# out_det = KNN(n_neighbors=5)\n",
    "# out_det = LOCI()\n",
    "# out_det = LOF()\n",
    "# out_det = LSCP()\n",
    "# out_det = MCD()\n",
    "# out_det = OCSVM(gamma=0.1)\n",
    "# out_det = PCA(n_components=10, whiten=True)\n",
    "# out_det = SOS()\n",
    "# out_det = XGBOD()\n",
    "out_det = AutoEncoder(hidden_neurons=[16,4,16], dropout_rate=0.4, \n",
    "                      validation_size=0.2,\n",
    "#                       epochs=100, \n",
    "#                       l2_regularizer=0.,\n",
    "                      verbose=2)\n",
    "out_det.fit(X);\n",
    "# scores = out_det.decision_scores_\n",
    "\n",
    "####\n",
    "# SPLIT HERE\n",
    "####\n",
    "\n",
    "clf = out_det\n",
    "\n",
    "modelname = 'AE'\n",
    "method='linear'\n",
    "\n",
    "def score(X):\n",
    "    return clf.predict_proba(X, method=method)[:,1]\n",
    "\n",
    "scores = score(X)\n",
    "stat_descr(scores, quantiles=[0, 0.05, 0.25, 0.5, 0.75, 0.95, 1.])\n",
    "\n",
    "bins = np.histogram(scores, 30)[1]\n",
    "fig, axes = plt.subplots(2,2, figsize=(14,10))\n",
    "# axes[0].hist(scores, bins=bins, histtype='step', lw=2, density=1, color='k');\n",
    "\n",
    "axes[0][0].hist(scores, bins=bins, histtype='step', lw=2, density=1, color='k');\n",
    "axes[0][0].set_title('normalized');\n",
    "\n",
    "axes[0][1].hist(scores, bins=bins, histtype='step', lw=2, density=0, color='k');\n",
    "axes[0][1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[0][1].set_title('unnormalized (log y)');\n",
    "\n",
    "axes[1][0].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=1, color='b');\n",
    "axes[1][0].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=1, color='r');\n",
    "axes[1][0].set_title('normalized by class');\n",
    "\n",
    "axes[1][1].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=0, color='b');\n",
    "axes[1][1].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=0, color='r');\n",
    "axes[1][1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[1][1].set_title('unnormalized (log y)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "\n",
    "axes[0].hist(scores, bins=bins, histtype='step', lw=2, density=1, color='k');\n",
    "axes[0].set_title('normalized');\n",
    "\n",
    "axes[1].hist(scores, bins=bins, histtype='step', lw=2, density=0, color='k');\n",
    "axes[1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[1].set_title('unnormalized (log y)');\n",
    "\n",
    "fig.savefig(f'graphics/{modelname}_{method}_bw.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "\n",
    "axes[0].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=1, color='b');\n",
    "axes[0].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=1, color='r');\n",
    "axes[0].set_title('normalized by class');\n",
    "\n",
    "axes[1].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=0, color='b');\n",
    "axes[1].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=0, color='r');\n",
    "axes[1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[1].set_title('unnormalized (log y)');\n",
    "\n",
    "fig.savefig(f'graphics/{modelname}_{method}_color.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute SHAPley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # kernel explainer with k-means\n",
    "k_in_kmeans = 10\n",
    "X_summary = shap.kmeans(X, k=k_in_kmeans)\n",
    "shap_explainer = shap.KernelExplainer(score, X_summary)\n",
    "shap_values = shap_explainer.shap_values(X);  # ~2-4 it/sec for k=5-20 in k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_summary is already stored in explainer.data\n",
    "# score_func is already stored in  explainer_kmeans.model.f\n",
    "\n",
    "# scores stored for validation of shap_explainer correct dumping\n",
    "\n",
    "\n",
    "\n",
    "description = f\"model={clf};scaler={scaler};shap_explainer={shap_explainer};k_in_kmeans={k_in_kmeans}\"\n",
    "\n",
    "model_data = dict(\n",
    "                  clf=clf, # has to be read into `clf` (like in `score` function), otherwise shap_explainer will be invalid\n",
    "                  X=X, y=y, scaler=scaler, col_names=col_names, \n",
    "                  fname_df_orig=fname_df_orig,\n",
    "                  shap_explainer=shap_explainer, shap_values=shap_values, k_in_kmeans=k_in_kmeans, \n",
    "                  scores=scores,\n",
    "                  description=description\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fname = 'storage/model_IF-1000_k10.dill.dill'\n",
    "with open(out_fname, 'wb') as f:\n",
    "    dill.dump(model_data, f, dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
