{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "import pyod\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "# from pyod.models.cof import COF\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.loci import LOCI\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.sos import SOS\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "shap.initjs()\n",
    "from helper.pdp import compute_pdp, plot_pdp, plot_ice\n",
    "from helper.utilis import stat_descr\n",
    "\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "import dill\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\", category=FutureWarning, module='pyod')\n",
    "warnings.filterwarnings(\"default\", category=FutureWarning, module='sklearn')\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA\n",
    "\n",
    "period = 'LHC18f'\n",
    "# train_on_good = True\n",
    "train_on_good = False\n",
    "scaler = StandardScaler(with_mean=False, with_std=False) \n",
    "\n",
    "\n",
    "### MODEL\n",
    "\n",
    "# out_det = ABOD(n_neighbors=5)\n",
    "# out_det = CBLOF(n_clusters=8)\n",
    "# out_det = HBOS(n_bins=10)\n",
    "out_det = IForest(10)\n",
    "# out_det = KNN(n_neighbors=100)\n",
    "# out_det = LOCI(alpha=0.005, k=3)\n",
    "# out_det = LOF()\n",
    "# out_det = LSCP()\n",
    "# out_det = MCD()\n",
    "# out_det = OCSVM(gamma=1e-5)\n",
    "# out_det = PCA(n_components=5, whiten=True)\n",
    "# out_det = SOS()\n",
    "# out_det = XGBOD()\n",
    "\n",
    "# hidden_neurons = [64,32,16,32,64]\n",
    "# dropout_rate = 0.2\n",
    "# epochs = 100\n",
    "# out_det = AutoEncoder(hidden_neurons=hidden_neurons, \n",
    "#                       dropout_rate=dropout_rate, \n",
    "#                       validation_size=0.2,\n",
    "#                       epochs=epochs, \n",
    "#                       l2_regularizer=0.,\n",
    "#                       verbose=2)\n",
    "\n",
    "\n",
    "### OTHER\n",
    "proba_method = 'unify'  # PyOD: predict_proba based on raw score\n",
    "compute_shap = False\n",
    "\n",
    "\n",
    "### OUTPUT\n",
    "\n",
    "description = f\"model={out_det};scaler={scaler};proba_method={proba_method};period={period}\"\n",
    "train_on_str = 'train-on-good' if train_on_good else 'train-on-all'\n",
    "shap_str = 'withSHAP' if compute_shap else 'noSHAP'\n",
    "if 'AutoEncoder' in out_det.__str__():\n",
    "    out_fname = f\"storage/model_{period}_AE-{'-'.join([str(hn) for hn in hidden_neurons])}_dropout{str(dropout_rate).replace('.', '')}_epochs{str(epochs)}_{train_on_str}_{shap_str}.dill\"\n",
    "else:\n",
    "    out_fname = f\"storage/model_{period}_IF-10_{train_on_str}_{shap_str}.dill\"\n",
    "\n",
    "print(out_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "fname_df_orig = f'data/trending_merged_{period}_withGraphs.csv'\n",
    "df_orig = pd.read_csv(fname_df_orig)\n",
    "\n",
    "target_col = 'alias_global_Warning'\n",
    "#----------\n",
    "\n",
    "df = df_orig[[c for c in df_orig.columns if \n",
    "              ('gr' not in c and 'alias' not in c and 'Unnamed' not in c)\n",
    "              and c != 'dataType.fString'\n",
    "              or c == target_col\n",
    "             ]]\n",
    "rename = lambda c: c if c != target_col else 'bad'\n",
    "df.columns = [rename(c) for c in df.columns]\n",
    "\n",
    "good_idx = df['bad'] == 0\n",
    "bad_idx  = df['bad'] == 1\n",
    "\n",
    "run_lst = df['run']\n",
    "id_lst = df['chunkID']\n",
    "start_lst = df['chunkStart']\n",
    "period_lst = df['period.fString']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_vars = pd.read_csv('data/sensitive_variables_list.csv', sep=':')\n",
    "sens_vars = [var[:-2] for var in sens_vars] + ['bad']\n",
    "\n",
    "df = df[sens_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "data = df.drop(['bad'], axis=1)\n",
    "data = data.drop(['oroc_A_side', 'oroc_C_side', 'iroc_A_side', 'iroc_C_side'], axis=1)\n",
    "x = data.to_numpy()\n",
    "x_s = scaler.fit_transform(x)\n",
    "X = pd.DataFrame(x_s, columns=data.columns)\n",
    "y = df['bad']\n",
    "col_names = X.columns\n",
    "\n",
    "if train_on_good: \n",
    "    out_det.fit(X[y==0]);\n",
    "else:\n",
    "    out_det.fit(X);\n",
    "\n",
    "# end of training\n",
    "\n",
    "    \n",
    "\n",
    "def score(X):   # used also in SHAP values computing\n",
    "    return out_det.predict_proba(X, method=proba_method)[:,1]\n",
    "\n",
    "scores = score(X)\n",
    "stat_descr(scores, quantiles=[0, 0.05, 0.25, 0.5, 0.75, 0.95, 1.])\n",
    "\n",
    "bins = np.histogram(scores, 30)[1]\n",
    "fig, axes = plt.subplots(2,2, figsize=(14,10))\n",
    "\n",
    "axes[0][0].hist(scores, bins=bins, histtype='step', lw=2, density=1, color='k');\n",
    "axes[0][0].set_title('normalized');\n",
    "\n",
    "axes[0][1].hist(scores, bins=bins, histtype='step', lw=2, density=0, color='k');\n",
    "axes[0][1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[0][1].set_title('unnormalized (log y)');\n",
    "\n",
    "axes[1][0].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=1, color='b');\n",
    "axes[1][0].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=1, color='r');\n",
    "axes[1][0].set_title('normalized by class');\n",
    "\n",
    "axes[1][1].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=0, color='b');\n",
    "axes[1][1].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=0, color='r');\n",
    "axes[1][1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[1][1].set_title('unnormalized (log y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute SHAPley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_shap:\n",
    "    # kernel explainer with k-means\n",
    "    k_in_kmeans = 10\n",
    "    X_summary = shap.kmeans(X, k=k_in_kmeans)\n",
    "    shap_explainer = shap.KernelExplainer(score, X_summary, l1_reg='num_features(10)')\n",
    "    shap_values = shap_explainer.shap_values(X);  # ~2-4 it/sec for k=5-20 in k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_summary is already stored in explainer.data\n",
    "# score_func is already stored in  explainer_kmeans.model.f\n",
    "\n",
    "# scores stored for validation of shap_explainer correct dumping\n",
    "\n",
    "\n",
    "\n",
    "model_data = dict(\n",
    "                  clf=out_det, # LEGACY, has to be read into `clf` (like in `score` function), otherwise shap_explainer will be invalid\n",
    "                  out_det=out_det, \n",
    "                  X=X, y=y, scaler=scaler, col_names=col_names, \n",
    "                  fname_df_orig=fname_df_orig,\n",
    "                  scores=scores,\n",
    "                  description=description,\n",
    "                  proba_method=proba_method,\n",
    "                  run_lst = run_lst,\n",
    "                  id_lst = id_lst,\n",
    "                  start_lst = start_lst,\n",
    "                  period_lst = period_lst\n",
    "                 )\n",
    "if compute_shap: \n",
    "    model_data['shap_explainer'] = shap_explainer\n",
    "    model_data['shap_values'] = shap_values\n",
    "    model_data['k_in_kmeans'] = k_in_kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(out_fname, 'wb') as f:\n",
    "    dill.dump(model_data, f, dill.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
