{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "import pyod\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.loci import LOCI\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.lscp import LSCP\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.sos import SOS\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "shap.initjs()\n",
    "from helper.pdp import compute_pdp, plot_pdp, plot_ice\n",
    "from helper.utilis import stat_descr\n",
    "\n",
    "from os import path\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "import dill\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\", category=FutureWarning, module='pyod')\n",
    "warnings.filterwarnings(\"default\", category=FutureWarning, module='sklearn')\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 'LHC18f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = dill.load(open(f'storage/model_LHC18f_IF-10_train-on-all_noSHAP.dill', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf            = model_data['clf']\n",
    "X              = model_data['X']\n",
    "y              = model_data['y']\n",
    "scaler         = model_data['scaler']\n",
    "col_names      = model_data['col_names']\n",
    "description    = model_data['description']\n",
    "fname_df_orig  = model_data['fname_df_orig']\n",
    "scores         = model_data['scores']\n",
    "proba_method   = model_data['proba_method']\n",
    "\n",
    "has_shap = 'shap_explainer' in model_data.keys()\n",
    "if has_shap:\n",
    "    shap_explainer = model_data['shap_explainer']\n",
    "    shap_values    = model_data['shap_values'] \n",
    "    k_in_kmeans    = model_data['k_in_kmeans']\n",
    "    assert all(shap_explainer.model.f(X) == scores)\n",
    "    print((shap_explainer.model.f(X) - scores).max())\n",
    "    \n",
    "cols = ['scores', 'period_lst', 'run_lst', 'id_lst', 'start_lst', 'y']\n",
    "df_model = pd.DataFrame(np.array([model_data[col] for col in cols]).transpose(), columns=[c.replace('_lst', '').replace('id', 'chunk') for c in cols])\n",
    "\n",
    "df_orig = pd.read_csv(fname_df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_idx = y == 0\n",
    "bad_idx  = y == 1\n",
    "\n",
    "scores = clf.predict_proba(X, method=proba_method)[:,1]\n",
    "\n",
    "bins = np.histogram(scores, 50)[1]\n",
    "fig, axes = plt.subplots(2,2, figsize=(14,10))\n",
    "\n",
    "axes[0][0].hist(scores, bins=bins, histtype='step', lw=2, density=1, color='k');\n",
    "axes[0][0].set_title('normalized');\n",
    "\n",
    "axes[0][1].hist(scores, bins=bins, histtype='step', lw=2, density=0, color='k');\n",
    "axes[0][1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[0][1].set_title('unnormalized (log y)');\n",
    "\n",
    "axes[1][0].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=1, color='b');\n",
    "axes[1][0].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=1, color='r');\n",
    "axes[1][0].set_title('normalized by class');\n",
    "\n",
    "axes[1][1].hist(scores[good_idx], bins=bins, histtype='step', lw=2, density=0, color='b');\n",
    "axes[1][1].hist(scores[bad_idx], bins=bins, histtype='step', lw=2, density=0, color='r');\n",
    "axes[1][1].set_yscale(\"log\", nonposy='clip')\n",
    "axes[1][1].set_title('unnormalized (log y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join model's training data and fitting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.DataFrame({'run':[123,123,123,124,124,125,111], 'chunk':[1,2,3,1,2,1,1], 'dummy1':[11,12,13,14,15,16,100]})\n",
    "# df2 = pd.DataFrame({'run':[123,123,123,124,124,125,222], 'chunk':[1,2,3,1,2,1,1], 'dummy2':[21,22,23,24,25,26,200]})\n",
    "# # pd.concat([df1,df2], join='outer', keys=['chunk', 'run'])\n",
    "# df1 = df1.set_index(['run', 'chunk'], drop=False)\n",
    "# df2 = df2.set_index(['run', 'chunk'], drop=False)\n",
    "# # df1\n",
    "# df12 = df1.join(df2, how='outer',\n",
    "#          lsuffix='', rsuffix='_2')\n",
    "# df12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "nbins = 100\n",
    "df_fits = pd.read_csv(f'data_validation_V0s/fitting/fit_results_{period}.csv').query('nbins == @nbins')\n",
    "\n",
    "fail_condition = 'mu < 495 or counts < 1000 or bound == True or sigma < 3'\n",
    "potential_fails = df_fits.query(fail_condition)\n",
    "df_fits = df_fits.query(f'not ({fail_condition})')\n",
    "for _, row in potential_fails.iterrows():\n",
    "    run, chunk = row['run'], row['chunk']\n",
    "    plot_name = f'data_validation_V0s/fitting/plots/{period}/fit_K0s_{period}_{run:.0f}_{chunk:03.0f}_nbins{nbins}.png'\n",
    "    print(plot_name)\n",
    "    if path.isfile(plot_name): display(Image(filename=plot_name))\n",
    "    else: print(f'no such file: {plot_name}')\n",
    "        \n",
    "print('dataframe of failed fits (failed based on `fail_condition`)')\n",
    "potential_fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fits.set_index(['run', 'chunk'], inplace=True, drop=False)\n",
    "df_model.set_index(['run', 'chunk'], inplace=True, drop=False)\n",
    "df_merged = df_model.join(df_fits, how='inner',\n",
    "                          lsuffix='', rsuffix='_fromFit')\n",
    "print('columns in merged df: ', df_merged.columns.tolist())\n",
    "print('merged df')\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be no difference between `global_Warning` from both dataframes  \n",
    "`y` is from model df and `bad` from fitting df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[['y', 'bad']].query('y != bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V0 valiadation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_V0(q_thresh):\n",
    "    #     q_thresh = 0.98\n",
    "    thresh_val = np.quantile(df_merged['scores'], q_thresh)\n",
    "    print(f'threshold quantile={q_thresh:.3f} \\t threshold value={thresh_val:.3f} ')\n",
    "    pred_bool_lst = [1 if sc >= thresh_val else 0 for sc in df_merged['scores']]\n",
    "\n",
    "    df_merged['scores_2'] = pred_bool_lst\n",
    "\n",
    "    sns.set(font_scale=1.3)\n",
    "    sns.set_style('white')\n",
    "    var_lst = ['mu', 'sigma', 'mu_err', 'counts']\n",
    "    sns.pairplot(df_merged.query('counts > 2000'), \n",
    "                 x_vars=var_lst,\n",
    "                 y_vars=var_lst,\n",
    "                 hue='scores_2', \n",
    "                 palette=np.array(sns.diverging_palette(260, 15, s=99, l=40, sep=1, n=15))[[5,-1]],\n",
    "                 aspect=1, height=2.5,\n",
    "                 diag_kws=dict(bw='silverman'),\n",
    "                 plot_kws=dict(s=50, edgecolor='w', alpha=0.7))\n",
    "    \n",
    "wg_q_thresh = widgets.FloatSlider(description='threshold quantile', min=0.7, max=1, value=0.95, step=0.01, continuous_update=False)\n",
    "ui_q = widgets.HBox([wg_q_thresh,])\n",
    "out = widgets.interactive_output(plot_V0, {'q_thresh': wg_q_thresh})\n",
    "display(ui_q, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score2cat(score):\n",
    "    perc = percentileofscore(df_merged['scores'], score, 'mean')\n",
    "    if   perc < 50: return 0\n",
    "    elif perc < 80: return 1\n",
    "    elif perc < 90: return 2\n",
    "    elif perc < 94: return 3\n",
    "    elif perc < 96: return 4\n",
    "    elif perc < 98: return 5\n",
    "    else:           return 6\n",
    "score2cat_labels = {'0':'< 50', '1':'50-80', '2':'80-90', '3':'90-94', '4':'94-96', '5':'96-98', '6':'> 98'}\n",
    "    \n",
    "df_merged['scores_2'] = df_merged.apply(lambda row: score2cat(row['scores']), axis=1)\n",
    "plt.hist([percentileofscore(df_merged['scores'], a, 'mean') for a in df_merged['scores']], bins=20)\n",
    "plt.xlabel('score percentiles')\n",
    "plt.figure()\n",
    "plt.hist(df_merged['scores_2'], bins=20)\n",
    "plt.xlabel('bins of score percentiles')\n",
    "\n",
    "\n",
    "g = sns.pairplot(df_merged, \n",
    "             x_vars=['mu', 'sigma', 'mu_err',  'counts'],\n",
    "             y_vars=['mu', 'sigma', 'mu_err',  'counts'],\n",
    "             hue='scores_2', \n",
    "#              palette=np.array(sns.diverging_palette(260, 15, s=99, l=40, sep=1, n=11))[-7:],\n",
    "             palette='coolwarm',\n",
    "             aspect=1, height=2.5,\n",
    "             diag_kws=dict(bw='silverman'),\n",
    "             plot_kws=dict(s=50, edgecolor='w', alpha=0.8))\n",
    "\n",
    "for t in g._legend.texts: t.set_text(score2cat_labels[t.get_text()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
