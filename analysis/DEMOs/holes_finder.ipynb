{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd; \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree, svm, neural_network, neighbors, ensemble\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback, TensorBoard\n",
    "\n",
    "from tensorflow.errors import InvalidArgumentError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "def printbold(string):\n",
    "    printmd(f'**{string}**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_row(n_holes, avail_holes_ids=None):\n",
    "    if avail_holes_ids == None: avail_holes_ids = [i for i in range(18)]\n",
    "    row = np.array([1]*18)\n",
    "    holes_id = np.random.choice( avail_holes_ids , size=n_holes, replace=False)\n",
    "    row[holes_id] = 0\n",
    "    return row\n",
    "\n",
    "generate_input_row(2, [0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_sample=1000, bad_fraction=0.25, n_holes=[1], avail_holes_ids=None):\n",
    "    def rand_n_holes(n_holes):\n",
    "        return np.random.choice(n_holes)\n",
    "    \n",
    "    good_rows_lst = [ generate_input_row(0) for _ in range(int(n_sample*(1-bad_fraction))) ]\n",
    "    bad_rows_lst  = [ generate_input_row(rand_n_holes(n_holes), avail_holes_ids=avail_holes_ids) for _ in range(int(n_sample*bad_fraction))]\n",
    "    good_target   = [1  for _ in range(int(n_sample*(1-bad_fraction)))]\n",
    "    bad_target    = [0 for _ in range(int(n_sample*bad_fraction))]\n",
    "    \n",
    "    \n",
    "    X = np.vstack(good_rows_lst + bad_rows_lst)\n",
    "    Y = good_target + bad_target\n",
    "    return X,Y\n",
    "\n",
    "generate_dataset(n_sample=20, bad_fraction=0.25, n_holes=[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(12, 8), sharey=True)\n",
    "holes_ids = range(18)\n",
    "ds = generate_dataset(n_sample=30, bad_fraction=1, avail_holes_ids=holes_ids)[0]\n",
    "axes[0].imshow(ds, cmap='binary_r')\n",
    "axes[0].set_title('uniform')\n",
    "\n",
    "holes_ids2 = [8,9,10,11]\n",
    "ds2 = generate_dataset(n_sample=30, bad_fraction=1, avail_holes_ids=holes_ids2)[0]\n",
    "axes[1].imshow(ds2, cmap='binary_r')\n",
    "axes[1].set_title('central sectors affected')\n",
    "\n",
    "holes_ids3 = [0,1,2,3]\n",
    "ds3 = generate_dataset(n_sample=30, bad_fraction=1, avail_holes_ids=holes_ids3)[0]\n",
    "axes[2].imshow(ds3, cmap='binary_r')\n",
    "axes[2].set_title('left sectors affected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uniform holes coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic sklearn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5)\n",
    "X_test,  y_test  = generate_dataset(n_sample=10000, bad_fraction=0.5)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "classifiers = [(svm.SVC(gamma='auto'), 'SVM'),\n",
    "               (tree.DecisionTreeClassifier(max_depth=3), 'DT-depth3'),\n",
    "               (tree.DecisionTreeClassifier(max_depth=16), 'DT-depth16'),\n",
    "               (tree.DecisionTreeClassifier(max_depth=20), 'DT-depth20'),\n",
    "               (neighbors.KNeighborsClassifier(n_neighbors=2), 'KNN-k2'),\n",
    "               (neighbors.KNeighborsClassifier(n_neighbors=15), 'KNN-k15'),\n",
    "               (ensemble.RandomForestClassifier(n_estimators=10, max_depth=2), 'RF-n10-depth2'),\n",
    "               (ensemble.RandomForestClassifier(n_estimators=1000, max_depth=2), 'RF-n1000-depth2'),\n",
    "               (ensemble.RandomForestClassifier(n_estimators=10, max_depth=8), 'RF-n10-depth8'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[6,], max_iter=200), 'NN-6-iter200'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[6,], max_iter=15000), 'NN-6-iter15000'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[20,]), 'NN-20'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[32,16,8,4]), 'NN-32-16-8-4'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[32,32,32]), 'NN-32-32-32')\n",
    "              ]\n",
    "\n",
    "\n",
    "\n",
    "for clf, descr in classifiers:\n",
    "    print(f'***{descr}***\\n')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f'Scores: accuracy = {acc}, ROC AUC = {auc}')\n",
    "    print(sklearn.metrics.confusion_matrix(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial conclusions\n",
    "Most classifiers can easily handle dataset with ___uniformly___ distributed holes. (Sometimes) Even extremely simple NN (4 hidden units in 1 layer) assuming sufficiently long training.  \n",
    "Only exceptions are tree-based classifiers if tree's depth is < 18 (incl. huge RF) -- each level exludes holes in one sector -- see graph below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "clf_dt = tree.DecisionTreeClassifier(max_depth=6)\n",
    "clf_dt.fit(X_train, y_train)\n",
    "dot_data = tree.export_graphviz(clf_dt, out_file=None, \n",
    "                      feature_names=['feat'+str(i) for i in range(18)],  \n",
    "                      class_names=['bad', 'good'],  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, y_pred)\n",
    "# plt.plot(fpr, tpr, color='darkorange',\n",
    "#          lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.015])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-uniform holes coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic sklearn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holes_ids_train = [i for i in range(0,9)]\n",
    "holes_ids_test  = [i for i in range(0,18)]\n",
    "X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_train)\n",
    "X_test,  y_test  = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "\n",
    "classifiers = [(svm.SVC(gamma='auto'), 'SVM'),\n",
    "               (tree.DecisionTreeClassifier(max_depth=3), 'DT-depth3'),\n",
    "               (tree.DecisionTreeClassifier(max_depth=16), 'DT-depth16'),\n",
    "               (tree.DecisionTreeClassifier(max_depth=20), 'DT-depth20'),\n",
    "               (neighbors.KNeighborsClassifier(n_neighbors=2), 'KNN-k2'),\n",
    "               (neighbors.KNeighborsClassifier(n_neighbors=15), 'KNN-k15'),\n",
    "               (ensemble.RandomForestClassifier(n_estimators=10, max_depth=2), 'RF-n10-depth2'),\n",
    "               (ensemble.RandomForestClassifier(n_estimators=1000, max_depth=2), 'RF-n1000-depth2'),\n",
    "               (ensemble.RandomForestClassifier(n_estimators=10, max_depth=8), 'RF-n10-depth8'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[6,], max_iter=200), 'NN-6-iter200'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[6,], max_iter=15000), 'NN-6-iter15000'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[20,]), 'NN-20'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[32,16,8,4]), 'NN-32-16-8-4'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[32,32,32]), 'NN-32-32-32'),\n",
    "               (neural_network.MLPClassifier(hidden_layer_sizes=[32,32,16,8,4]), 'NN-64-64-64-64'),\n",
    "              ]\n",
    "\n",
    "\n",
    "\n",
    "for clf, descr in classifiers:\n",
    "    print(f'***{descr}***\\n')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f'Scores: accuracy = {acc}, ROC AUC = {auc}')\n",
    "    print(sklearn.metrics.confusion_matrix(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial conlusions\n",
    "None of the classifiers cannot *of course* recognise unseen data. Some larger NN do quite good job reaching >80% when trained on training sample containing holes in half of the sectors (expected result is then 75% = 50% from good classification of \"good\" examples and 25% from correct classification of half of \"bad\"). \n",
    "\n",
    "Maybe it is possible to train good classificator without explicit invariance (i.e. convolutions)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MLP(activation='relu', dropout=0.0, neurons_layers='2x64', input_shape=1,\n",
    "                 batch_size=64, lr=0.01, optimizer='Nadam',\n",
    "                 return_descr=False, use_as_subnet=False):\n",
    "    \n",
    "    # for 1-layer nets dropout is not used !!!\n",
    "    model = Sequential()\n",
    "    if 'x' in neurons_layers:\n",
    "        n_layers, n_hidden = [int(n) for n in neurons_layers.split('x')]\n",
    "        model.add(Dense(n_hidden, activation=activation, input_shape=input_shape))\n",
    "        for neurons in range(n_layers-1):\n",
    "            # add dropout before, not after dense layer,\n",
    "            # as there should be no dropout between two last layers\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(Dense(n_hidden, activation=activation))\n",
    "\n",
    "        if use_as_subnet:\n",
    "            return model\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "    else:\n",
    "        hidden_lst = [int(h) for h in neurons_layers.split('-')]\n",
    "        model.add(Dense(hidden_lst[0], activation=activation, input_shape=input_shape))\n",
    "        for n_hidden in hidden_lst[1:]:\n",
    "            # add dropout before, not after dense layer,\n",
    "            # as there should be no dropout between two last layers\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(Dense(n_hidden, activation=activation))\n",
    "\n",
    "        if use_as_subnet:\n",
    "            return model\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    if   optimizer == 'SGD': opt = optimizers.SGD(lr=lr)\n",
    "    elif optimizer == 'Adam': opt = optimizers.Adam(lr=lr)\n",
    "    elif optimizer == 'Nadam': opt = optimizers.Nadam(lr=lr)\n",
    "    else: opt = optimizer  # accept also optimizer objects\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    model_descr = 'struct=FC:{struct}_lr={lr}_dropout={dropout}__opt={opt}_act={act}_batchsize={batch_size}'.format(\n",
    "                                                                     struct=neurons_layers,\n",
    "                                                                     opt=str(opt.__class__.__name__),\n",
    "                                                                     lr=lr,\n",
    "                                                                     act=activation,\n",
    "                                                                     dropout=dropout,\n",
    "                                                                     batch_size=batch_size)\n",
    "    print(model_descr)\n",
    "    if return_descr:\n",
    "        return model, model_descr\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ConvNet(n_conv_layers=2, n_filters_first=128, n_filters_change='down',\n",
    "                      kernel_size=2, pool_size=2, strides=1,\n",
    "                      n_fc_layers=4, n_fc_units=64,\n",
    "                      activation='relu', dropout_fc=0.0, dropout_conv=0.0,\n",
    "                      input_shape=1, batch_size=64, lr=0.0003, optimizer='Nadam',\n",
    "                      return_descr=False, use_as_subnet=False):\n",
    "    ''' n_filters_change : string 'down' or 'up' or 'flat'\n",
    "            each next conv layer will have consecutively\n",
    "            2x less or 2x more or same number of filters\n",
    "        pool_size : int or None\n",
    "            if None then no MaxPooling layers will be used,\n",
    "            otherwise will be applied after each conv layer\n",
    "        use_as_subnet : bool, default=False\n",
    "            if True, then not compiled model without softmax layer is returned\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    # Conv Layers\n",
    "    model.add( Conv1D(n_filters_first, kernel_size, strides=strides,  activation=activation, padding='same', input_shape=input_shape) )\n",
    "    if pool_size:\n",
    "        model.add( MaxPooling1D(pool_size=pool_size) )\n",
    "    n_filters_prev = n_filters_first\n",
    "    for i in range(n_conv_layers-1):\n",
    "\n",
    "        if dropout_conv > 1e-5:\n",
    "            model.add( Dropout(dropout_conv) )\n",
    "\n",
    "        if n_filters_change == 'flat':\n",
    "            n_filters = n_filters_prev\n",
    "            n_filters_prev = n_filters\n",
    "        elif n_filters_change == 'down':\n",
    "            n_filters = int(n_filters_prev/2)\n",
    "            n_filters_prev = n_filters\n",
    "        elif n_filters_change == 'up':\n",
    "            n_filters = int(n_filters_prev*2)\n",
    "            n_filters_prev = n_filters\n",
    "        model.add( Conv1D(n_filters, kernel_size, strides=strides,  activation=activation, padding='same') )\n",
    "        if pool_size:\n",
    "            model.add( MaxPooling1D(pool_size=pool_size) )\n",
    "\n",
    "\n",
    "    model.add( Flatten() )\n",
    "\n",
    "    # Fully-Connected Layers\n",
    "    for i in range(n_fc_layers):\n",
    "        if dropout_fc > 1e-5:\n",
    "            model.add( Dropout(dropout_fc) )\n",
    "        model.add( Dense(n_fc_units, activation=activation) )\n",
    "\n",
    "    if use_as_subnet:\n",
    "        return model\n",
    "\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    if   optimizer == 'SGD': opt = optimizers.SGD(lr=lr)\n",
    "    elif optimizer == 'Adam': opt = optimizers.Adam(lr=lr)\n",
    "    elif optimizer == 'Nadam': opt = optimizers.Nadam(lr=lr)\n",
    "    else: opt = optimizer  # accept also optimizer objects\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    index_flatten = ['Flatten' in str(l) for l in model.layers].index(True)\n",
    "    strides_str = '' if strides == 1  else 's='+str(strides)\n",
    "    kernel_str = '' if kernel_size == 2 else 'k='+str(kernel_size)\n",
    "    kernel_strides_str = '' if not strides_str and not kernel_str else '({},{})'.format(kernel_str, strides_str)\n",
    "    pool_str = '' if pool_size == 2 else '(p={})'.format(pool_size)\n",
    "    structure = '{conv_maxpool}x{n_conv}[{shape_first}->{shape_last}]+Dense({n_fc_units})x{n_fc_layers}'.format(\n",
    "                        conv_maxpool='(Conv1D{}+MaxPool{})'.format(kernel_strides_str, pool_str) if pool_size else 'Conv1D'+kernel_strides_str,\n",
    "                        n_conv=n_conv_layers,\n",
    "                        shape_first=model.layers[0].output_shape[1:],\n",
    "                        shape_last=model.layers[index_flatten].input_shape[1:],\n",
    "                        n_fc_units=n_fc_units,\n",
    "                        n_fc_layers=n_fc_layers)\n",
    "    model_descr = 'struct={struct}_lr={lr}_dropouts=({dropout_conv},{dropout_fc})'.format(\n",
    "                                                                     struct=structure,\n",
    "                                                                     lr=lr,\n",
    "                                                                     dropout_conv=dropout_conv,\n",
    "                                                                     dropout_fc=dropout_fc)\n",
    "    print(model_descr)\n",
    "    if return_descr:\n",
    "        return model, model_descr\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holes_ids_train = [i for i in range(0,18,2)]\n",
    "holes_ids_test  = [i for i in range(0,18)]\n",
    "X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_train)\n",
    "X_test,  y_test  = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "X_val,  y_val  = generate_dataset(n_sample=1000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = shuffle(X_train, random_state=1), shuffle(X_test, random_state=1), shuffle(y_train, random_state=1), shuffle(y_test, random_state=1)\n",
    "\n",
    "Y_train = K.utils.np_utils.to_categorical(y_train)\n",
    "Y_test = K.utils.np_utils.to_categorical(y_test)\n",
    "Y_val = K.utils.np_utils.to_categorical(y_val)\n",
    "\n",
    "\n",
    "paramsets = [dict(dropout=0.0, neurons_layers='2x4', batch_size=64, lr=0.01, optimizer='Nadam',),\n",
    "             dict(dropout=0.0, neurons_layers='2x16', batch_size=64, lr=0.01, optimizer='Nadam',),\n",
    "             dict(dropout=0.0, neurons_layers='2x32', batch_size=64, lr=0.01, optimizer='Nadam',),\n",
    "             dict(dropout=0.0, neurons_layers='4x16', batch_size=64, lr=0.01, optimizer='Nadam',),\n",
    "             dict(dropout=0.0, neurons_layers='8x16', batch_size=64, lr=0.01, optimizer='Nadam',),\n",
    "             dict(dropout=0.0, neurons_layers='2x32', batch_size=64, lr=0.01, optimizer='Nadam',),\n",
    "             dict(dropout=0.0, neurons_layers='6x32', batch_size=64, lr=0.01, optimizer='Nadam',),\n",
    "             dict(dropout=0.0, neurons_layers='32-16-8-4', batch_size=64, lr=0.01, optimizer='Nadam',),\n",
    "             dict(dropout=0.0, neurons_layers='32-32-16-16-8-8-4', batch_size=64, lr=0.01, optimizer='Nadam',),\n",
    "            ]\n",
    "\n",
    "classifiers = []\n",
    "for paramset in paramsets:\n",
    "    for n_epochs in [5,20,100]:\n",
    "        model = KerasClassifier(build_fn=create_MLP, epochs=n_epochs, verbose=0, input_shape=[18,])\n",
    "        for lr in [0.01, 0.001]:\n",
    "            paramset['lr'] = lr\n",
    "            print(paramset)\n",
    "            model.set_params(**paramset)\n",
    "            n_l = paramset['neurons_layers']\n",
    "            descr = f'MLP_{n_l}_lr{lr}_epochs{n_epochs}'\n",
    "            classifiers.append((model, descr))\n",
    "#history = model.fit(X_train, y_train, validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "history_lst = []\n",
    "for clf, descr in classifiers:\n",
    "    print(f'***{descr}***\\n')\n",
    "    history = clf.fit(X_train, Y_train, validation_data=(X_val, Y_val))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f'Scores: accuracy = {acc}, ROC AUC = {auc}')\n",
    "    print(sklearn.metrics.confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    history_lst.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lst[15].model.summary()\n",
    "history_lst[15].history['val_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Conclusions\n",
    "<a id=\"section_ID\"></a>\n",
    "\n",
    "Some MLPs reaches expected accuracy value (75%), but it's curious that some of them get very high values (>90%).  \n",
    "\n",
    "*Hypothesis:* Possible reason for that is that unused columns (they have always value = 1) can in principle be used as replacement for bias term in weighted sum of the neurons. This could be check with:\n",
    "1. training same model couple times and check if results do not vary (according to above theorem it can/should change)\n",
    "2. introducing some noise to each column (even 0.2 should be enough) - then, unused column would differ from bias term and dependence on them should vanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lst, auc_lst = [], []\n",
    "\n",
    "for _ in range(10):\n",
    "    holes_ids_train = [i for i in range(0,18,2)]\n",
    "    holes_ids_test  = [i for i in range(0,18)]\n",
    "    X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_train)\n",
    "    X_test,  y_test  = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "    X_val,  y_val  = generate_dataset(n_sample=1000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = shuffle(X_train, random_state=1), shuffle(X_test, random_state=1), shuffle(y_train, random_state=1), shuffle(y_test, random_state=1)\n",
    "\n",
    "    Y_train = K.utils.np_utils.to_categorical(y_train)\n",
    "    Y_test = K.utils.np_utils.to_categorical(y_test)\n",
    "    Y_val = K.utils.np_utils.to_categorical(y_val)\n",
    "\n",
    "\n",
    "    model = KerasClassifier(build_fn=create_MLP, epochs=3, verbose=0, input_shape=[18,])\n",
    "    paramset = dict(dropout=0.0, neurons_layers='2x8', batch_size=64, lr=0.01, optimizer='Nadam',)\n",
    "    model.set_params(**paramset)\n",
    "\n",
    "    clf = model\n",
    "    history = clf.fit(X_train, Y_train, validation_data=(X_val, Y_val))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f'Scores: accuracy = {acc}, ROC AUC = {auc}')\n",
    "    print(sklearn.metrics.confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    acc_lst.append(acc)\n",
    "    auc_lst.append(auc)\n",
    "    \n",
    "print(f'\\n\\taccuracy = {np.mean(acc_lst)} +/- {np.std(acc_lst)} \\t max={np.max(acc_lst)}\\n\\t ROC AUC = {np.mean(auc_lst)} +/- {np.std(auc_lst)} \\t max={np.max(auc_lst)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test 1 results**:  \n",
    "Test 1 is rather unconclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**without noise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=range(0,18,2))\n",
    "plt.imshow(X_train[-50:], cmap='binary_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lst, auc_lst = [], []\n",
    "\n",
    "for it in range(10):\n",
    "    holes_ids_train = [i for i in range(0,18,2)]\n",
    "    holes_ids_test  = [i for i in range(0,18)]\n",
    "    X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_train)\n",
    "    X_test,  y_test  = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "    X_val,  y_val  = generate_dataset(n_sample=1000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = shuffle(X_train, random_state=1), shuffle(X_test, random_state=1), shuffle(y_train, random_state=1), shuffle(y_test, random_state=1)\n",
    "\n",
    "    Y_train = K.utils.np_utils.to_categorical(y_train)\n",
    "    Y_test = K.utils.np_utils.to_categorical(y_test)\n",
    "    Y_val = K.utils.np_utils.to_categorical(y_val)\n",
    "\n",
    "#     X_train = X_train.astype('float64') + np.random.randn(*X_train.shape)*0.15\n",
    "#     X_test  = X_test.astype('float64')  + np.random.randn(*X_test.shape)*0.15\n",
    "#     X_val   = X_val.astype('float64')   + np.random.randn(*X_val.shape)*0.15\n",
    "    \n",
    "    model = KerasClassifier(build_fn=create_MLP, epochs=np.random.choice([5,15,25]), verbose=2, input_shape=[18,])\n",
    "    paramset = dict(dropout=np.random.choice([0, 0, 0, 0.1, 0.25]), \n",
    "                    neurons_layers=np.random.choice(['2x4', '2x8', '4x8', '4x16', '4x32', '6x8']), \n",
    "                    batch_size=64, \n",
    "                    lr=np.random.choice([1e-3, 3e-3, 1e-2]), \n",
    "                    optimizer='Nadam',)\n",
    "    model.set_params(**paramset)\n",
    "\n",
    "    clf = model\n",
    "    history = clf.fit(X_train, Y_train, validation_data=(X_val, Y_val))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f'iter={it}\\n {paramset}\\n\\n')\n",
    "    print(f'Scores: accuracy = {acc}, ROC AUC = {auc}')\n",
    "    print(sklearn.metrics.confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    acc_lst.append(acc)\n",
    "    auc_lst.append(auc)\n",
    "    print(f\"{'+--'*30}\")\n",
    "    \n",
    "print(f'\\n\\taccuracy = {np.mean(acc_lst)} +/- {np.std(acc_lst)} \\t max={np.max(acc_lst)}\\n\\t ROC AUC = {np.mean(auc_lst)} +/- {np.std(auc_lst)} \\t max={np.max(auc_lst)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with noise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=range(0,18,2))\n",
    "X_train = X_train.astype('float64') + np.random.randn(*X_train.shape)*0.15\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.imshow(X_train[-50:], cmap='binary_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lst, auc_lst = [], []\n",
    "\n",
    "for it in range(30):\n",
    "    holes_ids_train = [i for i in range(0,18,2)]\n",
    "    holes_ids_test  = [i for i in range(0,18)]\n",
    "    X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_train)\n",
    "    X_test,  y_test  = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "    X_val,  y_val  = generate_dataset(n_sample=1000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = shuffle(X_train, random_state=1), shuffle(X_test, random_state=1), shuffle(y_train, random_state=1), shuffle(y_test, random_state=1)\n",
    "\n",
    "    Y_train = K.utils.np_utils.to_categorical(y_train)\n",
    "    Y_test = K.utils.np_utils.to_categorical(y_test)\n",
    "    Y_val = K.utils.np_utils.to_categorical(y_val)\n",
    "\n",
    "    X_train = X_train.astype('float64') + np.random.randn(*X_train.shape)*0.15\n",
    "    X_test  = X_test.astype('float64')  + np.random.randn(*X_test.shape)*0.15\n",
    "    X_val   = X_val.astype('float64')   + np.random.randn(*X_val.shape)*0.15\n",
    "    \n",
    "    model = KerasClassifier(build_fn=create_MLP, epochs=np.random.choice([5,15,25]), verbose=2, input_shape=[18,])\n",
    "    paramset = dict(dropout=np.random.choice([0, 0, 0, 0.1, 0.25]), \n",
    "                    neurons_layers=np.random.choice(['2x4', '2x8', '4x8', '4x16', '4x32', '6x8']), \n",
    "                    batch_size=64, \n",
    "                    lr=np.random.choice([1e-3, 3e-3, 1e-2]), \n",
    "                    optimizer='Nadam',)\n",
    "    model.set_params(**paramset)\n",
    "\n",
    "    clf = model\n",
    "    history = clf.fit(X_train, Y_train, validation_data=(X_val, Y_val))\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(f'iter={it}\\n {paramset}\\n\\n')\n",
    "    print(f'Scores: accuracy = {acc}, ROC AUC = {auc}')\n",
    "    print(sklearn.metrics.confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    acc_lst.append(acc)\n",
    "    auc_lst.append(auc)\n",
    "    print(f\"{'+--'*30}\")\n",
    "    \n",
    "print(f'\\n\\taccuracy = {np.mean(acc_lst)} +/- {np.std(acc_lst)} \\t max={np.max(acc_lst)}\\n\\t ROC AUC = {np.mean(auc_lst)} +/- {np.std(auc_lst)} \\t max={np.max(auc_lst)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST 2 results:**  \n",
    "MLP without noise can quite easily learn to distinguish bad and good examples reaching acc of 80% and more  \n",
    "MLP seems to not be capable to achieve more than 75% (on the test set) if trained on dataset with noise even though train accuracy is often > 90-95%  \n",
    "\n",
    "[Hypothesis](#section_ID) was probably correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**  \n",
    "try various holes distribution in traininig sample, like range(0,18,2), range(0,9), (1,1,0,0,1,1,0,0..) etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history_lst_conv = []\n",
    "acc_lst, auc_lst = [], []\n",
    "\n",
    "\n",
    "for it in range(20):\n",
    "    print(f'iter = {it}')\n",
    "    holes_ids_train = [i for i in range(0,9)]\n",
    "    holes_ids_test  = [i for i in range(0,18)]\n",
    "    X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_train)\n",
    "    X_test,  y_test  = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "    X_val,  y_val  = generate_dataset(n_sample=1000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = shuffle(X_train, random_state=1), shuffle(X_test, random_state=1), shuffle(y_train, random_state=1), shuffle(y_test, random_state=1)\n",
    "\n",
    "    # add noise\n",
    "    X_train = X_train.astype('float64') + np.random.randn(*X_train.shape)*0.1\n",
    "    X_test  = X_test.astype('float64')  + np.random.randn(*X_test.shape)*0.1\n",
    "    X_val   = X_val.astype('float64')   + np.random.randn(*X_val.shape)*0.1\n",
    "    \n",
    "    Y_train = K.utils.np_utils.to_categorical(y_train)\n",
    "    Y_test = K.utils.np_utils.to_categorical(y_test)\n",
    "    Y_val = K.utils.np_utils.to_categorical(y_val)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, 2)\n",
    "    X_val = np.expand_dims(X_val, 2)\n",
    "    X_test = np.expand_dims(X_test, 2)\n",
    "\n",
    "\n",
    "    pset = dict(    filters1 = np.random.choice([10,25,100]),\n",
    "                    #strides1 = [1],\n",
    "                    kernel_size1 = [1],\n",
    "                    #filters2 = 0,\n",
    "                    filters2 = np.random.choice([0,0,5,10,20]),\n",
    "#                     strides1 = [np.random.choice([1,1,1,2,3,4])],\n",
    "#                     strides2 = [np.random.choice([1,2,3,4,6])],\n",
    "#                     kernel_size1 = [np.random.choice([1,2,3,4,6,9])],\n",
    "                    kernel_size2 = [np.random.choice([1,2,3,4])],\n",
    "                    max_pool1 = [np.random.choice([0,0,2,3,4])],\n",
    "                    max_pool2 = [np.random.choice([0,0,2,3,4])],\n",
    "                    #max_pool2 = [0],\n",
    "                    epochs = np.random.choice([5,15,25,50]),\n",
    "                    lr = np.random.choice([3e-5, 1e-4, 3e-4]),\n",
    "                    )\n",
    "\n",
    "    \n",
    "    print(f'\\n paramset = {pset}\\n')\n",
    "    try:\n",
    "        model = Sequential()\n",
    "        # Conv Layers\n",
    "        model.add( Conv1D(filters=pset['filters1'], \n",
    "                          kernel_size=pset['kernel_size1'], \n",
    "#                           strides=pset['strides1'], \n",
    "                          strides=pset['kernel_size1'],\n",
    "                          padding='same', activation='relu', input_shape=[18,1]) \n",
    "                 )\n",
    "        if pset['max_pool1'][0]: model.add( MaxPooling1D(pool_size=pset['max_pool1']) )\n",
    "        if pset['filters2']: model.add( Conv1D(filters=pset['filters2'], \n",
    "                                               kernel_size=pset['kernel_size2'], \n",
    "#                                                strides=pset['strides2'], \n",
    "                                               strides=pset['kernel_size2'],\n",
    "                                               padding='same', activation='relu')\n",
    "                                      )\n",
    "        if pset['max_pool2'][0]: model.add( MaxPooling1D(pool_size=pset['max_pool2']) )\n",
    "        model.add( Flatten() )\n",
    "\n",
    "        model.add( Dense(16, activation='relu') )\n",
    "        model.add( Dense(16, activation='relu') )\n",
    "        model.add( Dense(8, activation='relu') )\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizers.Nadam(lr=pset['lr']), metrics=['accuracy'], )\n",
    "    except InvalidArgumentError:\n",
    "        print('InvalidArgumentError!!')\n",
    "        continue       \n",
    "    except ValueError:\n",
    "        print('ValueError!!')\n",
    "        continue\n",
    "\n",
    "    clf = model          \n",
    "    history = clf.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=pset['epochs'], verbose=2)\n",
    "\n",
    "    y_pred_proba = [p[1] for p in clf.predict_proba(X_test)]\n",
    "    y_pred = [1 if p > 0.5 else 0 for p in y_pred_proba]\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    printmd(f'\\nScores: accuracy = {acc}, ROC AUC = **{auc}**')\n",
    "    print(sklearn.metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(f'\\n{\"+-\"*50}\\n')\n",
    "    history_lst_conv.append(dict(history=history, paramset=pset, acc=acc, auc=auc))\n",
    "    acc_lst.append(acc)\n",
    "    auc_lst.append(auc)\n",
    "\n",
    "          \n",
    "print(f'\\n\\taccuracy = {np.mean(acc_lst)} +/- {np.std(acc_lst)} \\t max={np.max(acc_lst)}\\n\\t ROC AUC = {np.mean(auc_lst)} +/- {np.std(auc_lst)} \\t max={np.max(auc_lst)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = 0.7222295454545454 +/- 0.06880813766469993 \t max=0.8767  \n",
    "ROC AUC = 0.7653125054545454 +/- 0.0994628708587779 \t max=0.9289947  \n",
    "Wall time: 21min 38s  \n",
    "\n",
    "accuracy = 0.7754347826086957 +/- 0.055096284169629514 \t max=0.8818  \n",
    "ROC AUC = 0.8188514591304348 +/- 0.07894300614814957 \t max=0.9635026200000001  \n",
    "Wall time: 59min 31s  \n",
    "\n",
    "accuracy = 0.7426666666666667 +/- 0.06864766727444001 \t max=0.8299  \n",
    "ROC AUC = 0.7832579722222222 +/- 0.08914965431386786 \t max=0.9762938799999998  \n",
    "Wall time: 31min 43s  \n",
    "\n",
    "accuracy = 0.7621090909090907 +/- 0.09308272424955667 \t max=0.959  \n",
    "ROC AUC = 0.805930949090909 +/- 0.12715505881597186 \t max=0.9970582  \n",
    "Wall time: 16min 4s  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`paramset = {'filters1': 20, 'filters2': 10, 'strides2': [3], 'kernel_size1': [3], 'kernel_size2': [3], 'max_pool1': [0], 'max_pool2': [2], 'epochs': 50, 'lr': 3e-05}, padding='same'`\n",
    "\n",
    "\n",
    "`paramset = {'filters1': 10, 'filters2': 10, 'strides2': [2], 'kernel_size1': [9], 'kernel_size2': [1], 'max_pool1': [2], 'max_pool2': [0], 'epochs': 25, 'lr': 0.0003}, padding='same'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONV 1D - WTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holes_ids_train = [i for i in range(0,9)]\n",
    "holes_ids_test  = [i for i in range(0,18)]\n",
    "X_train, y_train = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_train)\n",
    "X_test,  y_test  = generate_dataset(n_sample=10000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "X_val,  y_val  = generate_dataset(n_sample=1000, bad_fraction=0.5, avail_holes_ids=holes_ids_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = shuffle(X_train, random_state=1), shuffle(X_test, random_state=1), shuffle(y_train, random_state=1), shuffle(y_test, random_state=1)\n",
    "\n",
    "# add noise\n",
    "X_train = X_train.astype('float64') + np.random.randn(*X_train.shape)*0.15\n",
    "X_test  = X_test.astype('float64')  + np.random.randn(*X_test.shape)*0.15\n",
    "X_val   = X_val.astype('float64')   + np.random.randn(*X_val.shape)*0.15\n",
    "\n",
    "Y_train = K.utils.np_utils.to_categorical(y_train)\n",
    "Y_test = K.utils.np_utils.to_categorical(y_test)\n",
    "Y_val = K.utils.np_utils.to_categorical(y_val)\n",
    "\n",
    "X_train = np.expand_dims(X_train, 2)\n",
    "X_val = np.expand_dims(X_val, 2)\n",
    "X_test = np.expand_dims(X_test, 2)\n",
    "\n",
    "\n",
    "pset = dict(    filters1 = 20,\n",
    "                strides1 = [1],\n",
    "                kernel_size1 = [3],\n",
    "#                 filters2 = 0,\n",
    "                filters2 = 20,\n",
    "#                     strides1 = [np.random.choice([1,1,1,2,3,4])],\n",
    "#                     strides2 = [np.random.choice([1,2,3,4,6])],\n",
    "#                     kernel_size1 = [np.random.choice([1,2,3,4,6,9])],\n",
    "                kernel_size2 = [3],\n",
    "                strides2 = [1],\n",
    "                max_pool1 = [0],\n",
    "                max_pool2 = [0],\n",
    "                #max_pool2 = [0],\n",
    "                epochs = 15,\n",
    "                lr = 3e-4,\n",
    "                )\n",
    "\n",
    "\n",
    "print(f'\\n paramset = {pset}\\n')\n",
    "try:\n",
    "    model = Sequential()\n",
    "    # Conv Layers\n",
    "    model.add( Conv1D(filters=pset['filters1'], \n",
    "                      kernel_size=pset['kernel_size1'], \n",
    "                      strides=pset['strides1'], \n",
    "#                       strides=pset['kernel_size1'],\n",
    "                      padding='same', activation='relu', input_shape=[18,1]) \n",
    "             )\n",
    "    if pset['max_pool1'][0]: model.add( MaxPooling1D(pool_size=pset['max_pool1']) )\n",
    "    if pset['filters2']: model.add( Conv1D(filters=pset['filters2'], \n",
    "                                           kernel_size=pset['kernel_size2'], \n",
    "                                            strides=pset['strides2'], \n",
    "#                                            strides=pset['kernel_size2'],\n",
    "                                           padding='same', activation='relu')\n",
    "                                  )\n",
    "    if pset['max_pool2'][0]: model.add( MaxPooling1D(pool_size=pset['max_pool2']) )\n",
    "    model.add( Flatten() )\n",
    "\n",
    "#     model.add( Dense(16, activation='relu', input_shape=[18,]) )\n",
    "    model.add( Dense(16, activation='relu') )\n",
    "    model.add( Dense(16, activation='relu') )\n",
    "    model.add( Dense( 8, activation='relu') )\n",
    "    model.add( Dense( 2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Nadam(lr=pset['lr']), metrics=['accuracy'], )\n",
    "except InvalidArgumentError:\n",
    "    print('InvalidArgumentError!!')\n",
    "except ValueError:\n",
    "    print('ValueError!!')\n",
    "\n",
    "clf = model          \n",
    "history = clf.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=pset['epochs'], verbose=2)\n",
    "\n",
    "y_pred_proba = [p[1] for p in clf.predict_proba(X_test)]\n",
    "y_pred = [1 if p > 0.5 else 0 for p in y_pred_proba]\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "auc = sklearn.metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "printmd(f'\\nScores: accuracy = {acc}, ROC AUC = **{auc}**')\n",
    "print(sklearn.metrics.confusion_matrix(y_test, y_pred))\n",
    "print(f'\\n{\"+-\"*50}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.015])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.axis('equal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "val = pd.DataFrame(dict(pred=y_pred_proba, true=y_test))\n",
    "plt.hist(val.query('true==0')['pred'], bins=np.linspace(0,1,30), histtype='step', color='red', lw=2);\n",
    "plt.hist(val.query('true==1')['pred'], bins=np.linspace(0,1,30), histtype='step', lw=3, ls=':');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in history_lst_conv:\n",
    "    if exp['auc'] > 0.85:\n",
    "        printmd(f\"**{exp['auc']}**,  {exp['paramset']['filters1']}, {exp['paramset']['kernel_size1']}, {exp['paramset']['max_pool1']}, {exp['paramset']['filters2']}, {exp['paramset']['epochs']}\")\n",
    "#         printmd(f\"**{exp['auc']}**,  {exp['paramset']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "def get_val(x):\n",
    "    from collections.abc import Iterable\n",
    "    if isinstance(x, Iterable): return x[0]\n",
    "    else: return x\n",
    "\n",
    "for k in history_lst_conv[0]['paramset'].keys():\n",
    "    col = [get_val(exp['paramset'][k])  for exp in history_lst_conv]\n",
    "    data[k] = col \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['acc'] = [exp['acc'] for exp in history_lst_conv]\n",
    "df['auc'] = [exp['auc'] for exp in history_lst_conv]\n",
    "df.query('acc > 0.8').head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars = ['strides1', 'auc']\n",
    "xy = df.query('acc > 0')[['kernel_size1', 'auc']]\n",
    "xx = xy[xy.columns[0]]\n",
    "yy = xy[xy.columns[1]]\n",
    "xx = np.array(xx) + (np.random.rand(len(xx))-0.5)*0.3\n",
    "\n",
    "plt.scatter(xx, yy, edgecolors='b', facecolors='none')\n",
    "# plt.xlim([1e-5, 1e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in history_lst_conv:\n",
    "    if h['auc'] > 0.9:\n",
    "        printmd(f\"**AUC = {h['auc']}**, acc = {h['acc']}  \\n{h['paramset']}  \\n{h['history'].model.count_params()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial conclusions:\n",
    "\n",
    "ConvNets can learn to identify holes in regions not corrupted in the training sample in case of training holes randomly selected from `range(0,18,2)`\n",
    "\n",
    "**to be confirmed for `range(0,9)`** (~83% achieved so far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
