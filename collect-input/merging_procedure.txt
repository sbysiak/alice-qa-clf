# # # #
# CURRENT
# # # #

# create local directories per run e.g. OUTPUTS/data/2018/LHC18e/000286454/pass1/ @lxplus
# mind the leading "/"
bash create_dirs.sh "/alice/data/2018/LHC18d"

# create xmls @lxplus       LHC18o: 3min
bash create_xmls.sh "data/2018/LHC18d/"

# split xmls @lxplus        LHC18o: 30min
for xml in `find OUTPUTS/data/2018/LHC18d/ -name "files_to_be_merged.xml"`; do python split_xmls.py $xml; done

# create directories per time interval @alien
# AND
# copy xmls_per_time_interval from lxplus to alien @lxplus        LHC18o: ~2h
alien_main="/alice/cern.ch/user/s/sbysiak/grid-jobs/"
for xml in `find OUTPUTS/data/2018/LHC18d/ -name "files_to_be_merged_*.xml"`; do
    echo $xml
    dir_to_create=$(echo $xml | sed "s/files_to_be_merged/time_interval/" | sed "s/.xml//")
    alien_mkdir $alien_main$dir_to_create
    alien_cp file:$xml alien:$alien_main$dir_to_create
done

# submit jobs @lxplus
submit_jobs.sh  "OUTPUTS/data/2018/LHC18d" "pp"

# to resubmit (really submit again) failed jobs:
# first create list of jobs with status DONE or any from "in progress", e.g. RUNNING
# and then just run `submit_jobs.sh`
list_submitted_jobs.sh
submit_jobs.sh "data/2018/LHC18d"


---- large XMLs / PbPb ----
# some xmls will be invalid due to alien_find limitations
# error codes:
#   1 - Maximum number of columns exceeded (limit of alien_find reached)
#   124 - timeout - can be extended in create_xmls.sh
# after create_xmls.sh check log file in order to find runs with invalid xmls
# for those runs script which exec. alien_find for 3 parts and merges them in single xml
# then mv it to suitable dir
bash create_large_xml.sh "295589"

# split xmls into part1 and part2
bash split_in_two.sh       # set MAIN_DIR

# mkdir on alien and cp xmls there
bash submit_jobs_part.sh   # with cmd = cmd_mkdir + cmd_cp
# submit jobs
bash submit_jobs_part.sh   # with cmd = cmd_submit






---- after jobs are finished ----

# copy from grid to EOS
copy_jobs_output.sh "data/2018/LHC18d/"

# merging part_1 + part_2
# cd period-dir-on-eos
for f1 in `find . -name "QAresults.root" | grep part_1`; do
    f2=`echo $f1 | sed 's/part_1/part_2/g'`;
    fmerged=`echo $f1 | sed 's/part_1\///g'`;
    if [[ ! -e $f2 ]]; then echo "$f2 does not exists -- skip"; continue; fi;
    if [[ -e  $fmerged ]]; then echo "$fmerged found -- skip"; continue; fi;
    alihadd $fmerged $f1 $f2 ;
done

# on PLGRID:
python master_trending_jobs.py  # set params of main()

# hadd trending and copy to eos

# export trading to *.csv
aliroot -q "".x trending2csv.C(\"trending_merged_BlaBla.root\")"
python transpose.py trending_merged_BlaBla_transposed.csv




# BELOW - TODO

for f in `find . -name trending.root`; do
    echo $f;
    sshpass -f ~/.lxplus_pass rsync -avR $f sbysiak@lxplus.cern.ch:/eos/user/a/aliqat/www/qcml/data/;
done   # to be run from outside of alidock OR copy ~/.lxplus_pass file inside; check current path vs target one

# extract trading
make_trending.sh



if soft for trend extraction is modified, then run:

    cd $ALIBUILD_WORK_DIR/BUILD/AliPhysics-latest/AliPhysics
    cp ~/service-task/AliTPCPerformanceSummary.cxx /net/people/plgsbysiak/alice/AliPhysics/PWGPP/TPC/AliTPCPerformanceSummary.cxx
    cp ~/service-task/AliTPCPerformanceSummary.h /net/people/plgsbysiak/alice/AliPhysics/PWGPP/TPC/AliTPCPerformanceSummary.h
    make -j 4
    make install -j 4
    cp ~/.sw/slc7_x86-64/AliPhysics/master_ROOT6-1/PWGPP/TPC/macros/MakeTrend.C.modified  ~/.sw/slc7_x86-64/AliPhysics/master_ROOT6-1/PWGPP/TPC/macros/MakeTrend.C









# # # #
# DESIRED - to be done in the not so close future
# # # #

# create local directories per run e.g. OUTPUTS/data/2018/LHC18e/000286454/pass1/ @lxplus
create_local_dirs.sh YEAR PERIOD [RUN] [PASS]                   // <-TODO

# create xmls @lxplus
create_xmls.sh YEAR PERIOD [RUN] [PASS]                         // <-TODO

# split xmls @lxplus
split_xmls.py YEAR PERIOD [RUN] [PASS]                          // <-TODO

# create directories per time interval @alien
create_alien_dirs_per_time_interval.sh YEAR PERIOD [RUN] [PASS] // <- TODO

# copy xmls_per_time_interval from lxplus to alien @lxplus
copy_xmls_to_alien.sh YEAR PERIOD [RUN] [PASS]                  // <- TODO

# submit jobs @lxplus
submit_jobs.sh YEAR PERIOD [RUN] [PASS]

---- after jobs are finished ----

# extract trading

# export trading to *.csv
